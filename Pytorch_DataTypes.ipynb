{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Data Types\n",
        "There are 10 data types in the torch.tensor, some are used with CPUs and some with GPUS\n",
        "\n",
        "In this note book will get know data types and some methods to know inforamtion of a tensor."
      ],
      "metadata": {
        "id": "8RFsJcWXx8Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# lets start with 32-bit floating\n",
        "_32bit_tensor = torch.tensor(20, dtype=torch.float32) # here you have to give the object to as dtype\n",
        "print(\"32-bit datatype tensor: \", _32bit_tensor, _32bit_tensor.dtype)\n",
        "_32bit_cpu_tensor = torch.FloatTensor(1)\n",
        "print(\"32-bit-cpu datatype tensor: \", _32bit_cpu_tensor, _32bit_cpu_tensor.dtype, \"tensor device: \", _32bit_cpu_tensor.device)\n",
        "\n",
        "\"\"\"\n",
        "Before running torhc.cuda make sure the gpu is available in device.\n",
        "if you are using google collabe go to: Runtime->change rune time-> change cpu to gpu\n",
        "\"\"\"\n",
        "_32bit_gpu_tensor = torch.cuda.FloatTensor(1)\n",
        "print(\"32-bit-gpu datatype tensor: \", _32bit_gpu_tensor, _32bit_gpu_tensor.dtype, \"tensor device: \", _32bit_gpu_tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI5Po3iNybUm",
        "outputId": "02eb0c48-2f2d-41f8-c398-254723606f20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32-bit datatype tensor:  tensor(20.) torch.float32\n",
            "32-bit-cpu datatype tensor:  tensor([-5.8273e-17]) torch.float32 tensor device:  cpu\n",
            "32-bit-gpu datatype tensor:  tensor([0.], device='cuda:0') torch.float32 tensor device:  cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d2ecba713538>:12: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  _32bit_gpu_tensor = torch.cuda.FloatTensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets create 64 bit tensor\n",
        "_64bit_tensor = torch.tensor(20, dtype=torch.float64) # here you have to give the object to as dtype\n",
        "print(\"64-bit datatype tensor: \", _64bit_tensor, _64bit_tensor.dtype)\n",
        "_64bit_cpu_tensor = torch.DoubleTensor(1)\n",
        "print(\"64-bit-cpu datatype tensor: \", _64bit_cpu_tensor, _64bit_cpu_tensor.dtype, \"tensor device: \", _64bit_cpu_tensor.device)\n",
        "\n",
        "\"\"\"\n",
        "Before running torhc.cuda make sure the gpu is available in device.\n",
        "if you are using google collabe go to: Runtime->change rune time-> change cpu to gpu\n",
        "\"\"\"\n",
        "_64bit_gpu_tensor = torch.cuda.DoubleTensor(1)\n",
        "print(\"64-bit-gpu datatype tensor: \", _64bit_gpu_tensor, _64bit_gpu_tensor.dtype, \"tensor device: \", _64bit_gpu_tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4zAIoe00UdI",
        "outputId": "114bddc0-9496-4244-9ef7-3d8e9a16c50f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64-bit datatype tensor:  tensor(20., dtype=torch.float64) torch.float64\n",
            "64-bit-cpu datatype tensor:  tensor([4.9492e-310], dtype=torch.float64) torch.float64 tensor device:  cpu\n",
            "64-bit-gpu datatype tensor:  tensor([0.], device='cuda:0', dtype=torch.float64) torch.float64 tensor device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets create 16 bit tensor\n",
        "_16bit_tensor = torch.tensor(20, dtype=torch.half) # you can also use torch.float16\n",
        "print(\"64-bit datatype tensor: \", _16bit_tensor, _16bit_tensor.dtype)\n",
        "_16bit_cpu_tensor = torch.HalfTensor(1)\n",
        "print(\"64-bit-cpu datatype tensor: \", _16bit_cpu_tensor, _16bit_cpu_tensor.dtype, \"tensor device: \", _16bit_cpu_tensor.device)\n",
        "\n",
        "\"\"\"\n",
        "Before running torhc.cuda make sure the gpu is available in device.\n",
        "if you are using google collabe go to: Runtime->change rune time-> change cpu to gpu\n",
        "\"\"\"\n",
        "_16bit_gpu_tensor = torch.cuda.HalfTensor(1)\n",
        "print(\"64-bit-gpu datatype tensor: \", _16bit_gpu_tensor, _16bit_gpu_tensor.dtype, \"tensor device: \", _16bit_gpu_tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_vNt6IS0zjS",
        "outputId": "e5795809-4198-4c47-aa7d-89219443dffd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64-bit datatype tensor:  tensor(20., dtype=torch.float16) torch.float16\n",
            "64-bit-cpu datatype tensor:  tensor([0.2930], dtype=torch.float16) torch.float16 tensor device:  cpu\n",
            "64-bit-gpu datatype tensor:  tensor([0.], device='cuda:0', dtype=torch.float16) torch.float16 tensor device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# majorly will use the floating datatype. but for more will also explore the int datatype\n",
        "# now lets create 8 bit tensor\n",
        "_8bit_uint_tensor = torch.tensor(20, dtype=torch.uint8) # majorly this will use to create a blank image\n",
        "print(\"8-bit datatype tensor: \", _8bit_uint_tensor, _8bit_uint_tensor.dtype)\n",
        "\n",
        "_8bit_int_tensor = torch.CharTensor([20], )\n",
        "print(\"8-bit int datatype tensor: \", _8bit_int_tensor, _8bit_int_tensor.dtype)\n",
        "\n",
        "\"\"\"\n",
        "Before running torhc.cuda make sure the gpu is available in device.\n",
        "if you are using google collabe go to: Runtime->change rune time-> change cpu to gpu\n",
        "\"\"\"\n",
        "_8bit_int_gpu_tensor = torch.cuda.CharTensor(1)\n",
        "print(\"8-bit-gpu int datatype tensor: \", _8bit_int_gpu_tensor, _8bit_int_gpu_tensor.dtype, \"tensor device: \", _8bit_int_gpu_tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSj2tLo11POg",
        "outputId": "2a2dfd47-fd9d-4515-f26a-e507cefc2a5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8-bit datatype tensor:  tensor(20, dtype=torch.uint8) torch.uint8\n",
            "8-bit int datatype tensor:  tensor([20], dtype=torch.int8) torch.int8\n",
            "8-bit-gpu int datatype tensor:  tensor([0], device='cuda:0', dtype=torch.int8) torch.int8 tensor device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use below methods to create 16,32, 64\n",
        "print(\"16-bit int: torch.int16/torch.short\")\n",
        "print(\"16-bit cpu int: torch.ShortTesnor\")\n",
        "print(\"16-bit gpu int: torch.cuda.ShortTensor\")\n",
        "print(\"32-bit int: torch.int32\")\n",
        "print(\"32-bit cpu int: torch.IntTensor\")\n",
        "print(\"32-bit gpu int: torch.cuda.IntTensor\")\n",
        "print(\"64-bit int: torch.int64/torch.long\")\n",
        "print(\"64-bit cpu int: torch.LongTensor\")\n",
        "print(\"64-bit gpu int: torch.cuda.LongTensor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIZFllWM2NIn",
        "outputId": "3e1cfe9b-3d1d-480c-ab40-40e1eacffdb8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16-bit int: torch.int16/torch.short\n",
            "16-bit cpu int: torch.ShortTesnor\n",
            "16-bit gpu int: torch.cuda.ShortTensor\n",
            "32-bit int: torch.int32\n",
            "32-bit cpu int: torch.IntTensor\n",
            "32-bit gpu int: torch.cuda.IntTensor\n",
            "64-bit int: torch.int64/torch.long\n",
            "64-bit cpu int: torch.LongTensor\n",
            "64-bit gpu int: torch.cuda.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting information from tensor\n",
        "tensor = torch.cuda.ShortTensor([10])\n",
        "print(\"tensor and its device: \", tensor, tensor.device)\n",
        "print(\"lets check the datatype using tensor.dtype: \", tensor.dtype) # it will get the datatype information from the tensor\n",
        "\n",
        "# Note if tensor is scalar, metrix, vector or tensor the whole will store only with one data type.\n",
        "# ex: if you have a metix with 2 dime which contains 2 row, 3 columns total 6 value if you give float16 all value data type will be float16 or what you given data type.\n",
        "\n",
        "print(\"To know the tensor created and running on which device use tenor.device: \", tensor.device)\n",
        "# Note: Defualt when create any tensor it will run on the Cpu, to run Gpu need to use torch.cuda.Tensor to utilize the gpu."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzYRPqWJ3Nsn",
        "outputId": "12f068a7-2946-4389-acec-85775ba03c9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor and its device:  tensor([10], device='cuda:0', dtype=torch.int16) cuda:0\n",
            "lets check the datatype using tensor.dtype:  torch.int16\n",
            "To know the tensor created and running on which device use tenor.device:  cuda:0\n"
          ]
        }
      ]
    }
  ]
}