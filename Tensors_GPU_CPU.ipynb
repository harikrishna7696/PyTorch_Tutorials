{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversions\n",
        "\n",
        "Pytorch supports the conversion from numpy to torch tensors and tensors to numpy arrays, since numpy is popular library. Pytorch can easily interact with\n",
        "the numpy arrays\n",
        "\n",
        "1) Python list to tensor.\n",
        "\n",
        "2) Numpy to Tensor.\n",
        "\n",
        "3) Tensor to Numpy."
      ],
      "metadata": {
        "id": "V8nku_xntZkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "l-Yr0vMWuOVg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# python list to PyTorch\n",
        "\n",
        "py_li = [1 , 5, 8, 7, 9]\n",
        "print('python list: ', py_li, 'type: ', type(py_li))\n",
        "\n",
        "# 1) Easiest way to convert to tensor\n",
        "tensor = torch.tensor(py_li)\n",
        "print('python list to tensor: ', tensor, 'type: ', type(tensor))\n",
        "\n",
        "# 2) Convert to numpy, numpy-> tensor\n",
        "np_array = np.array(py_li)\n",
        "\n",
        "print('list to array: ', np_array, 'type: ', type(np_array))\n",
        "\n",
        "# torch.from_numpy(): using this method we can easily convert numpy to tensor\n",
        "tensor = torch.from_numpy(np_array)\n",
        "print('numpy array to tensor: ', tensor, 'type: ', type(tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTaHMybZunKp",
        "outputId": "f6b98a6d-6741-4b56-f388-b8794be45d9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python list:  [1, 5, 8, 7, 9] type:  <class 'list'>\n",
            "python list to tensor:  tensor([1, 5, 8, 7, 9]) type:  <class 'torch.Tensor'>\n",
            "list to array:  [1 5 8 7 9] type:  <class 'numpy.ndarray'>\n",
            "numpy array to tensor:  tensor([1, 5, 8, 7, 9]) type:  <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to retrieve the numpy array from tensor\n",
        "# tensor.numpy(): it will return the numpy array from tensor\n",
        "\n",
        "np_array = tensor.numpy()\n",
        "print('tensor to numpy array: ', np_array, 'type: ', type(np_array))\n",
        "\n",
        "\n",
        "# array to list\n",
        "py_list = np_array.tolist()\n",
        "print('python list: ', py_list, 'type: ', type(py_list))\n",
        "\n",
        "py_list1 = tensor.tolist()\n",
        "# or directily you can use tensor.tolist()\n",
        "print('python list: ', py_list1, 'type: ', type(py_list1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4ZbYGGGvQWj",
        "outputId": "eb358e04-a6a9-431c-f595-85331cfc64dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor to numpy array:  [1 5 8 7 9] type:  <class 'numpy.ndarray'>\n",
            "python list:  [1, 5, 8, 7, 9] type:  <class 'list'>\n",
            "python list:  [1, 5, 8, 7, 9] type:  <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Tensors on GPU(Graphical Processing Unit)\n",
        "\n",
        "which will make model training and inferencing very fast, when the model is training it will lot of operations on tensors, usually it done on CPU.\n",
        "which very slow, GPU will accelarate the training and inference part of the model.\n",
        "\n",
        "Using PyTorch you can build the model for GPU or CPU."
      ],
      "metadata": {
        "id": "VbkadxmCwLYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first lets check the is gpu is available or not, run the below command\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX3bT9aDyvK9",
        "outputId": "970c0418-c6a6-408a-9831-9db54e618f35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  3 10:02:43 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I am running in collab so collab has preinstalled nvidia change runtime from cpu to gpu\n",
        "# if you are linux run nvidia-smi\n",
        "# if the cuda is not availble try to install if GPU is available in you system"
      ],
      "metadata": {
        "id": "fah1GOwSy_T5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also check with PyTorch\n",
        "print(\"Is gpu available: \", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zumFwHsPzfje",
        "outputId": "c9eac182-1a81-47bd-c839-638c30d3be92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is gpu available:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# returned to true: means gpu is availble\n",
        "print(\"GPU's available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcNuRS2NzpEn",
        "outputId": "e206ea51-ad11-4aa8-ba1e-45d03198f1e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU's available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have only 1 GPU, Mulitple GPUs will make more faster training.\n",
        "# tensor.to(device), device='cpu' or 'cuda'\n",
        "\n",
        "tensor = torch.tensor(1)\n",
        "tensor, tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVJ82vMOz5k4",
        "outputId": "1555eec6-8057-4aa0-ebfd-89d3164ae360"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1), device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defulat tensor will be create on cpu.\n",
        "# to convert to gpu\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ACnIm5uR0OPQ",
        "outputId": "16aec202-f965-4584-fcdd-f296d2e89493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = tensor.to(device)\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn7gCNY20eef",
        "outputId": "c25cdfa2-8817-4f21-8063-fcdeb2664068"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from now the operations on this tensor will happend on gpu\n",
        "tensor = tensor + 10\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uHmLqSa0kjA",
        "outputId": "db96c2d1-e6ae-41c4-85f6-39dbad570ed2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can convert the model from cpu to gpu vice versa\n",
        "\n",
        "# from gpu to cpu\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oDy3FCA0wxw",
        "outputId": "eb746f41-531a-4231-c935-3ed13be1ebab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = tensor.cpu() # convert to numpy it will convert to cpu or use tensor.to('cpu')\n",
        "tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T7d4k5K19qC",
        "outputId": "6dda84bb-be56-463d-85c6-12529b099ed6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_xVbYQ72Fza",
        "outputId": "0ed1fd4f-9365-4c50-8de0-ee655c4ae705"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_gpu = tensor.cuda()\n",
        "to_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrtfcQOT2Nqg",
        "outputId": "8ea039b4-73d7-4eca-a851-5e1497f2b9d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_cpu = tensor.to('cpu')\n",
        "to_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAFtjhLa2U9I",
        "outputId": "a4256191-c244-4046-85de-2884f59b347c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}